{
  "name": "napari-vascilia",
  "display_name": "VASCilia",
  "visibility": "public",
  "icon": "",
  "categories": [],
  "schema_version": "0.1.0",
  "on_activate": null,
  "on_deactivate": null,
  "contributions": {
    "commands": [
      {
        "id": "napari-vascilia.initialize_ui",
        "title": "Initialize UI",
        "python_name": "napari_vascilia.Napari_VASCilia_v1_4_0:initialize_vascilia_ui",
        "short_title": null,
        "category": null,
        "icon": null,
        "enablement": null
      }
    ],
    "readers": null,
    "writers": null,
    "widgets": [
      {
        "command": "napari-vascilia.initialize_ui",
        "display_name": "VASCilia UI",
        "autogenerate": false
      }
    ],
    "sample_data": null,
    "themes": null,
    "menus": {},
    "submenus": null,
    "keybindings": null,
    "configuration": []
  },
  "package_metadata": {
    "metadata_version": "2.4",
    "name": "napari-vascilia",
    "version": "1.4.0",
    "dynamic": [
      "author",
      "author-email",
      "classifier",
      "description",
      "description-content-type",
      "home-page",
      "license",
      "license-file",
      "requires-dist",
      "summary"
    ],
    "platform": null,
    "supported_platform": null,
    "summary": "A plugin for deep learning-based 3D analysis of cochlear hair cell stereocilia bundles.",
    "description": "# VASCilia (Vision Analysis StereoCilia): A Napari Plugin for Deep Learning-Based 3D Analysis of Cochlear Hair Cell Stereocilia Bundles \n\n\n\n<p align=\"left\">\n\n  <img src=\"images/logo_3d.png\" alt=\"VASCilia Logo\" width=\"170\">\n\n</p>\n\n\n\n## Documentation\n\n\n\n**Check out the full documentation for this project at [this link](https://ucsdmanorlab.github.io/Napari-VASCilia/).**\n\n\n\nExplore the complexities of the cochlea with VASCilia, a Napari plugin created to aid in the 3D segmentation and quantification of stereocilia bundles. Equipped with a range of thoughtful features, VASCilia stands for (Vision Analysis StereoCilia) and it provides a supportive tool for auditory research, including:  \n\n1. Slice Selection: Easily navigate through 3D stacks to find the slices that matter most for your research.\n\n2. Stack Rotation: Adjust the orientation of your stack to facilitate better analysis.\n\n3. 3D Instance Segmentation: Identify and assess individual bundles with clear separation using deep learning.\n\n4. Bundle Deletion: Remove unwanted bundles to streamline your dataset.\n\n5. Regional Classification: identify whether the region is from BASE, MIDDLE, or APEX in the cochlea using deep learning.\n\n6. Hair Cell Differentiation: Distinguish between Inner Hair Cells and Outer Hair Cells with confidence using deep learning.\n\n7. Measurement Analysis: Calculate various measurements such as volume, centroid location, and surface area.\n\n8. Fluorescence Intensity Analysis: Assess the intensity of signal or protein with detailed precision.\n\n9. 3D Bundle Height Calculation: Measure the 3D distance from the peak to the base of each bundle, according to your sample's resolution.\n\n10. Bundle orientation: Determine bundle orientation for all hair cells based on two strategies: Height-only and Height&Distance.\n\n\n\nVASCilia &#x2764;&#xfe0f; is a valuable resource for the ear research community &#128066;, simplifying the complexity of measurement and analysis. It comes with a suite of pre-trained models to facilitate 3D segmentation, cell type identification and regional classification.\n\n\n\nFurthermore, we are committed to supporting research growth with a comprehensive training section for those looking to explore different staining techniques or develop new segmentation models through annotation and refinement.\n\n\n\nVASCilia is here to support researchers in their quest for deeper understanding and innovation in the study of cochlear structures.  \n\n*[click the image to see a highlights reel of the plugin](https://youtu.be/MwMOxJQ_elo)*  \n\n\n\n[![Watch the video](images/main_pipeline.png)](https://youtu.be/MwMOxJQ_elo)\n\n\n\n*[Click me to see a video demo of the entire workflow](https://youtu.be/mNPJ1g0vEW8)*  \n\n\n\n## How to install : \n\nSTEP1[Install WSL]:  \n\n1. Open the Command Prompt and install the Ubuntu 20.04 Distribution by simply copy paste this command  \n\nwsl --install -d Ubuntu-20.04\n\n2. After the setup successfully completes, reboot your computer. Open Ubuntu by typing \"Ubuntu\" in the search bar. A pop-up window for Ubuntu will appear. To check if CUDA and the GPU are correctly installed and available, type nvidia-smi in the terminal  \n\n\n\nSTEP2[Download the deep learning trained models]:\n\n1. Download the VASCilia_trained_models from https://www.dropbox.com/scl/fo/jsvldda8yvma3omfijxxn/ALeDfYUbiOuj69Flbc728rs?rlkey=mtilfz33qiizpul7uyisud5st&st=41kjlbw0&dl=0 \n\nnow you should have a folder called 'models'\n\n\n\n- \ud83d\udcc1 **models** `[Trained models]`\n\n    - \ud83d\udcc1 **cell_type_identification_model** `[has weights for cell type identification IHC vs OHC]`\n\n    - \ud83d\udcc1 **new_seg_model** `[incase you fine tune the existing model, the new model will be stored here]`\n\n    - \ud83d\udcc1 **region_prediction** `[has weights for region prediction]`\n\n    - \ud83d\udcc1 **seg_model**  `[has the weights for the 3D instance segmentation model]`\n\n    - \ud83d\udcc1 **Train_predict_stereocilia_exe** `[executible needed by the plugin to segment and retrain the model using WSL]`  \n\n    - \ud83d\udcc1 **ZFT_trim_model** `[deep learning model weights for z focus tracker algorithm]`  \n\n    - \ud83d\udcc1 **rotation_correction_model** `[deep learning model weights for correcting the orientation of the stack]`  \n\n \n\nSTEP3[download one dataset to test VASCilia]:  \n\ndownload one sample from our datasets to try in this link https://www.dropbox.com/scl/fo/pg3i39xaf3vtjydh663n9/h?rlkey=agtnxau73vrv3ism0h55eauek&dl=0  \n\ncreate a folder, called raw_data folder and put the downloaded dataset inside the raw_data folder\n\n\n\n  - \ud83d\udcc1 **raw_data** `[raw data (stacks) is placed here]`\n\n    - \ud83d\udcc4 Litter 12 Mouse 4 MIDDLE - delBUNdelCAP_Airyscan Processing.czi\n\n   \n\nAlso create another folder called processed_data in which the plugin will use to store the results of the analysis\n\n  \n\n  - \ud83d\udcc1 **processed_data** `[processed data will be stored here]`\n\n\n\n## Instructions for Cloning the Repository [You can do either Option A or Option B]:\n\n## Option A: From source (recommended for dev):  \n\n```sh\n\ngit clone https://github.com/ucsdmanorlab/Napari-VASCilia.git\n\ncd Napari-VASCilia\n\nconda create -y -n napari-VASCilia -c conda-forge python=3.10    \n\nconda activate napari-VASCilia    \n\npip install -r requirements.txt\n\npip install -e .\n\nnapari  \n\n```\n\n## Option B: From PyPI (end users):\n\n```sh\n\nconda create -y -n napari-VASCilia -c conda-forge python=3.10    \n\nconda activate napari-VASCilia \n\npip install --extra-index-url https://download.pytorch.org/whl/cu113 torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113\n\npip install napari-vascilia\n\nnapari  \n\n```\n\nPost-installation:  \n\n1. Activate the plugin through Plugins -> VASCilia UI (Napari-VASCilia).\n\n2. This will generate the config.json file at C:/Users/Username/.napari-vascilia/config.json. Update the paths in config.json as needed.\n\nconfig.json will be generated upon running the plugin for the first time.\n\n\n\n- \ud83d\udcc1 C:/Users/Username/   [your home folder]\n\n  - \ud83d\udcc1 **.napari-vascilia** `[Folder_path]`\n\n    - \ud83d\udcc4 **config.json**\n\n\n\nPlease update the /.../ portion according to your paths:\n\n\n\n```sh\n\n{\n\n    \"rootfolder\": \"C:/Users/.../processed_data/\",\n\n    \"wsl_executable\": \"C:/Users/.../models/Train_predict_stereocilia_exe/Train_Predict_stereocilia_exe_v2\",\n\n    \"model\": \"C:/Users/.../models/seg_model/stereocilia_v7/\",\n\n    \"model_output_path\": \"C:/Users/.../models/new_seg_model/stereocilia_v8/\",\n\n    \"model_region_prediction\": \"C:/Users/.../models/region_prediction/resnet50_best_checkpoint_resnet50_balancedclass.pth\",\n\n    \"model_celltype_identification\": \"C:/Users/.../models/cell_type_identification_model/\",\n\n    \"ZFT_trim_model\": \"C:/Users/.../models/ZFT_trim_model/\",\n\n    \"rotation_correction_model\": \"C:/Users/.../models/rotation_correction_model/\",\n\n    \"green_channel\": 0,\n\n    \"red_channel\": 1,\n\n    \"blue_channel\": -1,\n\n    \"signal_intensity_channel\": 0,\n\n    \"subtract_background\": True,\n\n    \"dilate_labels\": False,\n\n    \"flag_to_upscale\": False,\n\n    \"flag_to_downscale\": False,\n\n    \"flag_to_pad\": False,\n\n    \"resize_dimension\": 1500,\n\n    \"pad_dimension\": 2000,\n\n    \"force_manual_resolution\": 0,\n\n    \"button_width\": 60,\n\n    \"button_height\": 18\n\n}\n\n```\n\n\n\nCongratulations :) &#127881;, now you can enjoy working with the plugin. \n\n\n\n## Unique about VASCilia :  \n\nVASCilia saves all the intermediate results and the variables inside a pickle file while the user is using it in a very effiecint way. That allows a super fast uploading for the analysis if the user or their supervisor wants to keep working or review the analysis steps.  \n\n*[Click me to learn how to upload a z-stack](https://youtu.be/Sxm_fsjoWL0)*  \n\n\n\n## How to use VASCilia :  \n\n*[Click me to see a video demo of the entire workflow](https://youtu.be/mNPJ1g0vEW8)*  \n\n\n\nThere are several buttons inside the blugin in the right hand side of Napari:\n\n\n\n1. 'Open CZI Cochlea Files and Preprocess' button: read the CZI file.\n\n2. 'Upload Processed CZI Stack' button: Incase you already have processed the stack, then just uplead your Analysis_state.pkl that usually has all the variables needed to upload your analysis\n\n3. 'Trim Full Stack' button: this button allows you to choose only the slices of interest (has been automated in v_1_1_0)\n\n4. \"Rotate' buttom: this button allows to rotate the stack to have proper analysis (has been automated in v_1_1_0)  \n\n5. Segment with 3DBundleSeg: it is a two steps algorithm (2D detection + multi-object assignment algorithm across all slices) to produce robust 3D detection. 3DBundleSeg is the first instance segmentation model for stereocilia bundles in the literature. It is trained on P5 and P21 3D stacks (thousands of 2D instances) and it produces highly acccurate boundary delineation even in the most challenging datasets. Here are some examples:  \n\n\n\n<p align=\"center\">\n\n  <strong>3DBundleSeg can tackle challenged cases</strong>  \n\n  <br>\n\n  <img src=\"images/challenged_cases_gray.png\" width=\"100%\">\n\n</p>\n\n\n\n<p align=\"center\">\n\n  <strong>Multi-object assignment algorithm to produce robust 3D detection</strong>  \n\n  <br>\n\n  <img src=\"images/3DBundleSeg.png\" width=\"100%\">\n\n</p>\n\n\n\n\n\n6. Delete Label 'button': delete the unwanted detection if it is near the boundary or for any other reason.\n\n7. Calculate measurments 'button': calculate different measurments from the detected bundles and store them in csv file\n\n8. Calculate Bundle Height 'button': compute the 3D distance from the highest point in the 3D detection of each bundle to it's base. This calculation will consider the sample resolution.\n\n9. Perform Cell Clustering 'button': find the IHC, OHC1, OHC2, and OHC3 using either GMM, Kmeans or Deep Learning. Those layers will be added to the plugin to be used during the analysis. \n\n10. Compute Fluorescence Intensity 'button': produce plots and CSV files that has the accumelated intensity and mean intensity for the fluorescence signal.\n\n11. Predict Region 'button': Predict whether the region is from the BASE, MIDDLE, or APEX region using a RESNET50 trained model. \n\n12. Compute Orientation: It computes the orientation using two strategies.\n\n\n\n<p align=\"center\">\n\n  <strong>Bundle Height with top and bottom adjustable points in red and green, orientation with two points in magenta, and bundle ID in green</strong>  \n\n  <br>\n\n  <img src=\"images/Bundles.png\" width=\"50%\">\n\n</p>\n\n\n\n<p align=\"center\">\n\n  <strong>Cell type identification (IHC1 in yellow, OHC1 in cyan, OHC2 in green, and OHC3 in magenta)</strong>  \n\n  <br>\n\n  <img src=\"images/clustering.png\" width=\"50%\">\n\n</p>\n\n\n\n13. Training Section.\n\n\n\n<p align=\"center\">\n\n  <strong>Training section</strong>  \n\n  <br>\n\n  <img src=\"images/Training_section2.png\" width=\"40%\">\n\n</p>\n\n\n\nThe training section is for the research ear community incase their datasets are little different than ours then they can easily create their cround truth, train a new model and use it in the plugin\n\n1. Create/Save Ground Truth 'button': this button will create a new layer to draw new ground truth and save them as variables inside the plugin\n\n2. Generate Ground Truth Mask 'button': this button will save all the generated masks after finish annotating to a new folder. \n\n3. Display Stored Ground Truth 'button': this button will display the stored masks in the plugin.\n\n4. Copy Segmentation Masks to Ground Truth 'button': this button helps in speeding up the annotation process by copying what our trained model is producing sothat the annotator will only correct the wrong part.\n\n5. Move Ground Truth to Training Folder 'button': this button will move all the annotated ground truth to the training folder to start the training process. \n\n6. Check Training Data 'button': this button checks the training data whether they follow the format needed by the architecture. It checks whether there are training and valiation folders and it reads every single file to make sure it doesn't have redundant or no information. It gives warning messages incase it finds an issue.\n\n7. Train New Model for 3DBundleSeg 'button': this button will start the training.\n\n\n\nVASCilia also equipped with two more buttons for resetting (to facilitate transitions between analyzing several stacks) and also exit VASCilia.  \n\nWe are still working on the documentation, so this gihub will be continiuosly updated.\n\n\n\n## Multi-Batch Processing Feature: Required File\n\nThe **Multi-Batch Processing** feature in this package requires an additional file 'file_names_for_batch_processing.csv' to be in the same path of your rootfolder in your config file. \n\n### Download the File\n\nYou can download the csv file from the following link and don't forget to change the paths https://www.dropbox.com/scl/fo/pg3i39xaf3vtjydh663n9/h?rlkey=agtnxau73vrv3ism0h55eauek&dl=0  \n\n:\n\n\n\n## Testing Other Lab Data  \n\nLiberman Data *[Click me to see a video demo of the entire workflow](https://youtu.be/PIG3q7G6Xr0)*  \n\nArtur Indzhykulian Data *[Click me to see a video demo of the entire workflow](https://youtu.be/WseYK4Zn-3o)*  \n\n\n\n## Paper and Citation\n\n\n\nThis work will be submitted very soon. If you want to read or cite the paper &#128522;, you can find it [here](https://doi.org/10.1101/2024.06.17.599381).  \n\n\n\nKassim, Y. M., Rosenberg, D. B., Renero, A., Das, S., Rahman, S., Al Shammaa, I., Salim, S., Huang, Z., Huang, K., Ninoyu, Y., Friedman, R. A., Indzhykulian, A. A., & Manor, U. (2024). VASCilia (Vision Analysis StereoCilia): A Napari Plugin for Deep Learning-Based 3D Analysis of Cochlear Hair Cell Stereocilia Bundles. bioRxiv. https://doi.org/10.1101/2024.06.17.599381\n\n\n\n## Project Authors and Contacts\n\n\n\n**Python Implementation of this repository:** Dr. Yasmin M. Kassim    \n\n**Contact:** ykassim@ucsd.edu, ymkgz8@mail.missouri.edu  \n\nYasmin Kassim was responsible for the plugin design, fully implemented all functions in Python, wrote the manuscript,\n\nproofread the ground truth data, created all figures, and established the GitHub repository and codebase.\n\n\n\n**Stacks used in this study imaged by:** Dr. David Rosenberg   \n\n\n\n**Height bundle ground truth analyses**: Samprita Das and Alma Renero.  \n\n\n\n**StereoCilia Bundles Ground Truth**: 55 (P5 and P21) 3D stacks were manually annotated by Yasmin Kassim and five undergraduate students using the CVAT annotation tool. This is an extremely challenging process, as each 3D stack might have up to 60 bundles in a 3D setting, which could translate to around 1000 bundles in a 2D setting across all frames. The students involved in this effort are:  \n\n**Samia Rahman, Ibraheem Al Shammaa, Samer Salim, Zhuoling Huang, and Kevin Huang**.\n\n\n\nThis dataset will be the first annotated dataset in the literature to 3D segment the stereocilia bundles and it will be published and available for the ear research community with the publication of this paper.\n\n\n\n**Other Lab Support**:   \n\nYuzuru Ninoyu assisted with some of the imaging data, with Rick Friedman\u2019s supervision and support.   \n\nArtur Indzhykulian provided additional imaging data for testing.  \n\n\n\n**Lab Supervisor:** Dr. Uri Manor   \n\nThe Principal Investigator, conceived and supervised the project, and provided critical\n\nrevisions and updates to the manuscript.  \n\n\n\n**Contact:** u1manor@UCSD.EDU  \n\n**Department:** Cell and Development Biology Department/ UCSD  \n\n**Lab Website:** https://manorlab.ucsd.edu/\n\n\n\n\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "keywords": null,
    "home_page": "https://github.com/ucsdmanorlab/Napari-VASCilia",
    "download_url": null,
    "author": "Yasmin Kassim",
    "author_email": "ymkgz8@mail.missouri.edu",
    "maintainer": null,
    "maintainer_email": null,
    "license": "BSD-3-Clause",
    "classifier": [
      "Programming Language :: Python :: 3",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Framework :: napari"
    ],
    "requires_dist": [
      "numpy==1.26.4",
      "scikit-learn==1.3.2",
      "opencv-python",
      "matplotlib",
      "imagecodecs",
      "tifffile",
      "napari[all]",
      "readlif",
      "czitools==0.4.1",
      "npe2",
      "colormap==1.1.0",
      "segmentation-models-pytorch==0.3.3",
      "pretrainedmodels==0.7.4"
    ],
    "requires_python": null,
    "requires_external": null,
    "project_url": null,
    "provides_extra": null,
    "provides_dist": null,
    "obsoletes_dist": null
  },
  "npe1_shim": false
}